机器学习（吴达恩）
====
# PART2

## PART-2.1:监督学习:
指机器学习x→y的映射算法，特征是给予学习算法示例。
机器学习能从给定的正确的x与y搭配的映射种学习到特征，并且实现在不给定y只给x的情况下，得到合理的预测或猜测。

监督学习从给定数据标签中学习“正确答案”

### 算法：
回归：
![1](D:\note\ML\NotePic\1.png)

分类算法：集输出集合，不连续

![2](D:\note\ML\NotePic\2.png)

![3](D:\note\ML\NotePic\3.png)

## PART-2.2:无监督学习:
输入的数据没有标签，之所以称之为为监督学习是应为我们不试图监督算法。

定义：

在监督学习中，数据同时带有输入 x 和输入标签 y，而在无监督学习中，数据仅带有输入 x 而没有输出标签 y，并且算法必须找到数据中的*结构*。

![5](D:\note\ML\NotePic\5.png)

### 算法：
聚类算法：

![4](D:\note\ML\NotePic\4.png)

异常检测：寻找异常数据点。

降维：讲数据压缩，同时尽可能减少信息丢失。
#  PART3:
## PART-3.1:线性回归模型：
在分类中，只有少数可能的输出。
在回归中，可以输出无限多可能的数字。

![7](D:\note\ML\NotePic\7.png)



![6](D:\note\ML\NotePic\6.png)

训练集、xym，单个训练样本表示
## PART-3.2:COST函数：
衡量预测值尽可能的接近真实值，为构建一个不会随着训练集大小变大而增大的COST函数，计算平均平方误差而不是总平方误差（通过除以m实现）。
平方误差COST函数：（除以2是为了方便后续运算）

![8](D:\note\ML\NotePic\8.png)
### 可视化

![9](D:\note\ML\NotePic\9.png)

最小圆处损失最小

![10](D:\note\ML\NotePic\10.png)
# PART4:
## PART-4.1:梯度下降算法：

梯度下降算法：能最小化损失函数J。

![11](D:\note\ML\NotePic\11.png)

随机选取一个w和b，如果然后根据所”最陡“的方向迈出一步，再重复这个过程，直到走到最低处。如果选择的是另一个w和b，有可能会走到另一个最低处。

这几个最低处我们称为局部最优解。

![12](D:\note\ML\NotePic\12.png)



α:学习率（0-1之间的小整数），控制下坡幅度。

【如果α很小，会下降很慢；如果过大，可能会过冲，并且永远不会到达最小值，或者说，大交叉无法收敛，甚至可能发散。】

【可以理解为:J——向哪个方向迈出小步，α——下坡的步子大小】

![13](D:\note\ML\NotePic\13.png)

【即使α固定，随着下降过程，步长也会越来越小。】

重复，直到收敛（达到==局部最小值==，其中w和b不在随着每次采取额外步骤而发生太大变化。）

### 批量梯度下降（"Batch" gradient descent）:
指在梯度下降的每一步中，都在查看所有的训练示例，而不仅仅是训练数据的一个子集。

# WORK2
## PART-1.1 多维特征

![image-20231124165441642](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20231124165441642.png)

PS:上述算法可以被叫做多元回归

$x$与$w$都可以看作是行向量，有n个特征值就是有n个值的数组

## PART1.2 矢量化
（np中的dot函数）

![image-20231124170358161](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20231124170358161.png)

矢量化的两个优势：
（1）：代码更短
（2）：运算速度更快 【深层原因:NumPy的dot函数能在计算机中使用并行硬件，比for循环或者顺序计算效率更高】

具体说明:

![image-20231124170946111](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20231124170946111.png)
### numpy中数组、dot的使用方法
```python
import numpy as np    # it is an unofficial standard to use np for numpy
import time
```
np.zero：生成元素为0的向量
np.arange：生成元素不为0的向量
np.shape：查看维度，若使用np.shape(a[0])，可以查询行向量的列数
如果shape的对象是数，则返回值为空
```python
X = np.array([[1],[2],[3],[4]])
w = np.array([2])
c = np.dot(X[1], w)

print(f"X[1] has shape {X[1].shape}")
print(f"w has shape {w.shape}")
print(f"c has shape {c.shape}")
```
输出有
```
X[1] has shape (1,)
w has shape (1,)
c has shape ()
```
### Slicing：切割
【开始位置:结束:步长】
例如
```
a[] = [0 1 2 3 4 5 6 7 8 9]
a[2:7:1] =  [2 3 4 5 6]
```

## PART-1.3 多元回归的梯度下降

![image-20231205163857531](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20231205163857531.png)

![image-20231205163330850](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20231205163330850.png)

一种替代方法：正规方程

## PART-2.1 特征缩放

![7](C:\Users\HP\Desktop\note\note_pic\note_pic\7.png)

![image-20231206152119463](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20231206152119463.png)

==方法1==：除去最大值

例如：

$300\le x_1\le 2000$		

$x_{1,scaled}= x_1 /2000$



==方法2==：均值归一化

​		step1：计算各个特征数据的平均值$\mu_1$

​		step2：$x_1 = (x_1 -\mu_1)/x_{max} - x_{min}$

​		所以, $-0.18 \leq x_1 \leq 0.82$



==第三种方法==：Z-score归一化
		step:1 计算标准差$\sigma$和平均值$\mu$。

​		step2: eg. $300\leq x_1 \leq2000$

​							$x_1=(x_1-\mu_1)/\sigma_1$

​							所以, $-0.67 \leq x_1 \leq 3.1$



在特征缩放时， 我我们希望特征值$x_j$的范围为$[-1,1]$

![1](C:\Users\HP\Desktop\note\note_pic\note_pic\1.png)
通过该曲线可以找出何时开始训练特定的模型

确定模型何时完成的另一方法：自动收敛测试（对于$\epsilon$的选取可能相对困难）
	存在一个小量$\epsilon$(eg.0.001)
	如果在一次迭代中J的减小幅度$j\leq \epsilon$ 则可以看做是收敛的



### 缩放后与原始数据对比：

![8](C:\Users\HP\Desktop\note\note_pic\note_pic\8.png)	

### 选择适当的学习率

J出现上升的几种可能：
（1）BUG，可以使$\alpha$取一个非常非常小的值，如果此时J还在上升，可以认为代码有bug

（2）学习率太大（3）w迭代时的-号写成了+号

在选择$\alpha$的时候，可以先从小的开始增加，加到一个太大的值（j出现上升），则再将$\alpha$减小，保证其略小于出现J上升时的值即可。（学习率尽可能大）

```
有几点需要注意:有了多个特征，我们就不能再用一个图来显示结果和特征了。•在生成图时，使用归一化特征。使用从规范化训练集中学习到的参数的任何预测也必须进行规范化预测生成模型的目的是用它来预测数据集中没有的房价。
当训练数据被归一化时，必须使用得到的均值和标准差对数据进行归一化。
```

## PART-2.2 特征工程

用直觉去创建一个新特征

![2](C:\Users\HP\Desktop\note\note_pic\note_pic\2.png)

## PART-2.3多项式回归

选择特征的方法：

（1）取高次项（平方，立方项等），如

$f(x)=w_1x+w_2x^2+w_3x^3+b$

（2）取根号项，如

$f(x)=w_1x+w_2\sqrt{x}+b$

![3](C:\Users\HP\Desktop\note\note_pic\note_pic\3.png)

### 一个例子

```
x = np.arange(0,20,1)
y = np.cos(x/2)

X = np.c_[x, x**2, x**3,x**4, x**5, x**6, x**7, x**8, x**9, x**10, x**11, x**12, x**13]
X = zscore_normalize_features(X) 

model_w,model_b = run_gradient_descent_feng(X, y, iterations=1000000, alpha = 1e-1)

plt.scatter(x, y, marker='x', c='r', label="Actual Value"); plt.title("Normalized x x**2, x**3 feature")
plt.plot(x,X@model_w + model_b, label="Predicted Value"); plt.xlabel("x"); plt.ylabel("y"); plt.legend(); plt.show()
```

输出：

```
Iteration         0, Cost: 2.24887e-01
Iteration    100000, Cost: 2.31061e-02
Iteration    200000, Cost: 1.83619e-02
Iteration    300000, Cost: 1.47950e-02
Iteration    400000, Cost: 1.21114e-02
Iteration    500000, Cost: 1.00914e-02
Iteration    600000, Cost: 8.57025e-03
Iteration    700000, Cost: 7.42385e-03
Iteration    800000, Cost: 6.55908e-03
Iteration    900000, Cost: 5.90594e-03
w,b found by gradient descent: w: [-1.61e+00 -1.01e+01  3.00e+01 -6.92e-01 -2.37e+01 -1.51e+01  2.09e+01
 -2.29e-03 -4.69e-03  5.51e-02  1.07e-01 -2.53e-02  6.49e-02], b: -0.0073
```

![4](C:\Users\HP\Desktop\note\note_pic\note_pic\4.png)





## PART-2.4:库的使用

Scikit-Learn，目前使用相对较为广泛的库



## 一些QML的lab

![5](C:\Users\HP\Desktop\note\note_pic\note_pic\5.png)

```
https://github.com/krishnakumarsekar/awesome-quantum-machine-learning
```

![9](C:\Users\HP\Desktop\note\note_pic\note_pic\9.png)

```
https://github.com/mit-han-lab/torchquantum
```
